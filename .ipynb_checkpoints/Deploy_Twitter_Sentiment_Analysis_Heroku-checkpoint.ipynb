{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "# import joblib\n",
    "from joblib import dump\n",
    "# import joblib\n",
    "from joblib import load\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# To sort dictionary values\n",
    "import operator \n",
    "## Preprocessing\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import config\n",
    "    \n",
    "# initialize api instance\\n\n",
    "consumer_key= config.consumer_key\n",
    "consumer_secret= config.consumer_secret\n",
    "access_token=config.access_token\n",
    "access_token_secret =config.access_token_secret\n",
    "\n",
    "#Connect to Twitter through the API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret) \n",
    "api = tweepy.API(auth,wait_on_rate_limit=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Twitter Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trends_by_location(loc_id,count):\n",
    "    '''Get Trending Tweets by Location'''\n",
    "    import iso639\n",
    "    import numpy as np\n",
    "    from langdetect import detect\n",
    "    df = pd.DataFrame([])\n",
    "    try:\n",
    "        trends = api.trends_place(loc_id)\n",
    "        df = pd.DataFrame([trending['name'],  trending['tweet_volume'], iso639.to_name(detect(trending['name']))] for trending in trends[0]['trends'])\n",
    "        df.columns = ['Trends','Volume','Language']\n",
    "        #df = df.sort_values('Volume', ascending = False)\n",
    "        return(df[:count])\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        print(\"An exception occurred: \",e)\n",
    "        df = pd.DataFrame([trending['name'],  trending['tweet_volume'], np.nan] for trending in trends[0]['trends'])\n",
    "        df.columns = ['Trends','Volume','Language']\n",
    "        return(df[:count])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Worldwide Twitter Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trends</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#AskCuppyAnything</td>\n",
       "      <td>12405.0</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carrefour</td>\n",
       "      <td>407859.0</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taysom Hill</td>\n",
       "      <td>17839.0</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#StreamLifeGoesOn</td>\n",
       "      <td>152933.0</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#VidasNegrasImportam</td>\n",
       "      <td>87607.0</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#TransDayOfRemembrance</td>\n",
       "      <td>37338.0</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#TeröristSelo</td>\n",
       "      <td>15239.0</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mourão</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Geraldo</td>\n",
       "      <td>26063.0</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Edward Norton</td>\n",
       "      <td>11207.0</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Trends    Volume    Language\n",
       "0  #AskCuppyAnything       12405.0   English   \n",
       "1  Carrefour               407859.0  English   \n",
       "2  Taysom Hill             17839.0   Swedish   \n",
       "3  #StreamLifeGoesOn       152933.0  English   \n",
       "4  #VidasNegrasImportam    87607.0   Portuguese\n",
       "5  #TransDayOfRemembrance  37338.0   French    \n",
       "6  #TeröristSelo           15239.0   German    \n",
       "7  Mourão                 NaN        Portuguese\n",
       "8  Geraldo                 26063.0   German    \n",
       "9  Edward Norton           11207.0   English   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_world_trends = get_trends_by_location(1, 20)\n",
    "df_world_trends.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Translated Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation(text):\n",
    "    ''' Translate Tweets in English'''\n",
    "    from googletrans import Translator  # Import Translator module from googletrans package\n",
    "    try:\n",
    "        translator = Translator() # Create object of Translator.\n",
    "        translated = translator.translate(text)\n",
    "        return(translated.text)\n",
    "    except Exception as e:\n",
    "        #print(\"Exception in get_translation\", e)\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trends</th>\n",
       "      <th>Language</th>\n",
       "      <th>Translated_Trends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#AskCuppyAnything</td>\n",
       "      <td>English</td>\n",
       "      <td>#AskCuppyAnything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carrefour</td>\n",
       "      <td>English</td>\n",
       "      <td>Carrefour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Trends Language  Translated_Trends\n",
       "0  #AskCuppyAnything  English  #AskCuppyAnything\n",
       "1  Carrefour          English  Carrefour        "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_world_trends[\"Translated_Trends\"] = [get_translation(val) for val in df_world_trends.Trends]\n",
    "df_world_trends[[\"Trends\",\"Language\",\"Translated_Trends\"]].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Tweets for a Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_tweets(search_keyword):\n",
    "    ''' collect tweets '''\n",
    "    try: \n",
    "        count = 50\n",
    "        # Create Blank Dataframe\\n\",\n",
    "        df_tweets = pd.DataFrame(pd.np.empty((0, 1)))\n",
    "        for keyword in search_keyword:\n",
    "            # Remove Retweets\n",
    "            search_tag = keyword +  \"-filter:retweets\" +  \"-filter:media\"\n",
    "            \n",
    "            print('Searching tweets for: ', search_tag)\n",
    "    \n",
    "            fetched_tweets = tweepy.Cursor(api.search,\n",
    "                                q=search_tag,\n",
    "                                lang=\"en\").items(50)\n",
    "            # Add records to the dataframe\n",
    "            df_tweets = df_tweets.append([[tweet.text] for tweet in fetched_tweets])\n",
    "            # Add columns\n",
    "            df_tweets.columns = ['tweets']\n",
    "            #clean emojis and pictures from tweets\n",
    "            df_tweets['tweets'] = df_tweets['tweets'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "            # Retuen Data\n",
    "            return(df_tweets)\n",
    "    except Exception as e:\n",
    "        print('Encountered Exception:', e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Random Forest  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL friend.............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trailer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                                  SentimentText\n",
       "0  0                               is so sad for my APL friend.............\n",
       "1  0                             I missed the New Moon trailer...          "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "\n",
    "## ---------------------------------\n",
    "##Read Data \n",
    "## ---------------------------------\n",
    "    \n",
    "df = pd.read_csv(\"twitter_sentiments.csv\")\n",
    "\n",
    "# Delete Columns\n",
    "del df[\"SentimentSource\"]\n",
    "del df[\"Unnamed: 4\"]\n",
    "del df[\"Unnamed: 5\"]\n",
    "del df[\"Unnamed: 6\"]\n",
    "del df[\"ItemID\"]\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------------------------\n",
    "##Data Preprocessing \n",
    "# ---------------------------------\n",
    "# convert to lower case\n",
    "df['clean_tweet'] = df['SentimentText'].str.lower()\n",
    "# Remove punctuations\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace('[^\\w\\s]',' ')\n",
    "# Remove spaces in between words\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace(' +', ' ')\n",
    "# Remove Numbers\n",
    "df['clean_tweet'] = df['clean_tweet'].str.replace('\\d+', '')\n",
    "# Remove trailing spaces\n",
    "df['clean_tweet'] = df['clean_tweet'].str.strip()\n",
    "# Remove URLS\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "# remove stop words\n",
    "stop = stopwords.words('english')\n",
    "stop.extend([\"racism\",\"alllivesmatter\",\"amp\",\"https\",\"co\",\"like\",\"people\",\"black\",\"white\"])\n",
    "df['clean_tweet'] =  df['clean_tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\n",
    "\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------------------------\n",
    "##Prepare Random Forest Model\n",
    "##Define Pipeline Stages\n",
    "## ---------------------------------\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase= True, max_features=1000, stop_words=ENGLISH_STOP_WORDS)\n",
    "tfidf_vectorizer.fit(df.clean_tweet)\n",
    "\n",
    "# transform the train and test data,\n",
    "tweets_idf = tfidf_vectorizer.transform(df.clean_tweet)\n",
    "\n",
    "\n",
    "# create the object of Random Forest Model\n",
    "model_RF = RandomForestClassifier(n_estimators=100)\n",
    "model_RF.fit(tweets_idf, df.Sentiment)\n",
    "\n",
    "pipeline = Pipeline(steps= [('tfidf', TfidfVectorizer(lowercase=True,\n",
    "                                                      max_features=1000,\n",
    "                                                      stop_words= ENGLISH_STOP_WORDS)),\n",
    "                                                      ('model', RandomForestClassifier(n_estimators = 100))])\n",
    "\n",
    "# fit the pipeline model with the training data                            \n",
    "pipeline.fit(df.clean_tweet, df.Sentiment)\n",
    "    \n",
    "# save the model\n",
    "#dump(pipeline, open('model.pkl', 'wb'))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump the Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "from joblib import dump\n",
    "\n",
    "# dump the pipeline model\n",
    "dump(pipeline, filename=\"text_classification.joblib\", compress= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Emotion behind tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(tweets):\n",
    "    '''Predict Emotions behind tweets'''\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from joblib import load\n",
    "    try:\n",
    "        # load the model\n",
    "        # load the saved pipleine model\n",
    "        pipeline = load(\"text_classification.joblib\")\n",
    "        # get the prediction\n",
    "        tweets['Prediction'] = pipeline.predict(tweets['tweets'])\n",
    "        return tweets\n",
    "    except Exception as e:\n",
    "        print(\"Exception in predict_emotion: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df_tweets):\n",
    "    '''Clean the Tweets'''\n",
    "    # convert to lower case\n",
    "    df_tweets['clean_text'] = df_tweets['tweets'].str.lower()\n",
    "    # Remove punctuations\n",
    "    df_tweets['clean_text'] = df_tweets['clean_text'].str.replace('[^\\w\\s]',' ')\n",
    "    # Remove spaces in between words\n",
    "    df_tweets['clean_text'] = df_tweets['clean_text'].str.replace(' +', ' ')\n",
    "    # Remove Numbers\n",
    "    df_tweets['clean_text'] = df_tweets['clean_text'].str.replace('\\d+', '')\n",
    "    # Remove trailing spaces\n",
    "    df_tweets['clean_text'] = df_tweets['clean_text'].str.strip()\n",
    "    # Remove URLS\n",
    "    df_tweets['clean_text'] = df_tweets['clean_text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend([\"amp\",\"https\",\"co\",\"rt\",\"new\",\"let\",\"also\",\"still\",\"one\",\"people\",\"gt\"])\n",
    "    df_tweets['clean_text'] =  df_tweets['clean_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop ))\n",
    "\n",
    "    # Remove Text Column\n",
    "    del df_tweets['tweets']\n",
    "    # Rename the clean_text column as tweets\n",
    "    df_tweets.rename(columns = {'clean_text':'Tweets'}, inplace = True) \n",
    "    return(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function for a hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keyword = 'King Von'\n",
    "# Get tweets for a hashtag\n",
    "df_tweets = get_related_tweets(search_keyword)\n",
    "# Predict Emotion for the tweets\n",
    "df_tweets = predict_emotion(df_tweets)\n",
    "# Clean the tweets\n",
    "df_tweets = data_cleaning(df_tweets)\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "\n",
    "ax = df_tweets.Prediction.value_counts().plot(kind = \"bar\")\n",
    "ax.legend([\"Positive\",\"Negative\"],loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect webpage and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "from flask import Flask, render_template, request, redirect, url_for, Response\n",
    "from flask_table import Table, Col\n",
    "from joblib import load\n",
    "from flask import send_file\n",
    "#import base64\n",
    "from io import BytesIO\n",
    "#import io\n",
    "#import random\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import random, threading, webbrowser\n",
    "\n",
    "\n",
    "####------function to get trening tweets----####\n",
    "def gettrends():\n",
    "    # get the twitter trends\n",
    "    df_world_trends = get_trends_by_location(1, 10)\n",
    "    df_world_trends[\"Translated_Trends\"] = [get_translation(val) for val in df_world_trends.Trends]\n",
    "    return df_world_trends[[\"Trends\",\"Language\",\"Translated_Trends\"]]\n",
    "\n",
    "####------ function to get results for a particular text query----####\n",
    "def requestResults(search_keyword):\n",
    "    # get the tweets text\n",
    "    df_tweets = get_related_tweets(search_keyword)\n",
    "    # Predict Emotion for the tweets\n",
    "    df_tweets = predict_emotion(df_tweets)\n",
    "    # Clean the tweets\n",
    "    df_tweets = data_cleaning(df_tweets)\n",
    "   \n",
    "\n",
    "    return df_tweets\n",
    "\n",
    "####------ function to create a plot ----####\n",
    "def create_plot():\n",
    "    fig = Figure()\n",
    "    img = df_tweets.Prediction.value_counts().plot(kind = \"bar\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# start flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "####------render default webpage ----####\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('home.html')\n",
    "\n",
    "# when the post method detect, then redirect to trends page\n",
    "@app.route('/', methods=['POST', 'GET'])\n",
    "def get_trends():\n",
    "    if request.method == 'POST':\n",
    "        trends = gettrends()\n",
    "        return render_template('trends.html', table=trends.to_html())\n",
    "\n",
    "\n",
    "# when the post method detect, create a url for success\n",
    "@app.route('/get_data', methods=['POST', 'GET'])\n",
    "def get_data():\n",
    "    if request.method == 'POST':\n",
    "        name = request.form['search']\n",
    "        return redirect(url_for('success', name=name))\n",
    "\n",
    "#create a Plot and send a png file\n",
    "@app.route('/plot_png')\n",
    "def plot_png():\n",
    "    fig = create_plot(results)\n",
    "    img = BytesIO()\n",
    "    plt.savefig(img)\n",
    "    img.seek(0)\n",
    "    #plot_url = base64.b64encode(img.getvalue())\n",
    "    return send_file(img, mimetype='image/png')\n",
    "\n",
    "####------when the post method detect, then redirect to results page\n",
    "@app.route('/success/<name>')\n",
    "def success(name):\n",
    "    #return \"<xmp>\" + str(requestResults(name)) + \" </xmp> \"\n",
    "    results = requestResults(name)\n",
    "    plot_url = plot_png()\n",
    "    return render_template('results.html', img=plot_url, table=results.to_html())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #port = 5000 + random.randint(0, 999)\n",
    "    #print(port)\n",
    "    #url = \"http://127.0.0.1:{0}\".format(port)\n",
    "    #print(url)\n",
    "    #app.run(use_reloader=False, debug=True, port=port)\n",
    "    app.run(use_reloader=False, debug=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
